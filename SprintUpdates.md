Sprint 4 (Final) Updates:

Alberto Mancarella  

These few weeks, I focused on adding final features and polishing the app before presenting it! I reviewed NewsAPI and utilized it to fetch the top topics in the U.S. I achieved this by utilizing the NewsAPI get_top_headlines method in a Flask Python script. 
This Python script was implemented to get the content for these top headlines. Using this content, I implemented a ScrollView where the top headline title and its source are displayed for each article in this "Popular Topics" section of the app. I also worked on being able to analyze each individual article by clicking on the article itself in this ScrollView and dynamically transitioning to the "analysis" page of the pap. This was done by getting the URL (from NewsAPI) and sending it to our same pipeline, which analyzes an article based on URL input. In addition to this, I also worked on the “Popular News Sources” screen, building off of Lucy’s work. Previously, every single news source had its own javascript file (1 file for nbc, one for cnn, etc.). I implemented a  system where each news source now utilizes one javascript file.  In addition, I added the actual analysis of each news source Nico sent to the app. Last but not least,  I did extensive testing where I looked for bugs in time for the final presentation and fixed the bugs that I found. I added an alert feature if the URL for a top headline article is unavailable when attempting to analyze the article.

Sadly, the course is over, so there’s not much to update you on what I’ll be doing next, haha!


Nicolas Friley:

During sprint 4, I made modifications to the Python scripts to enhance flexibility in output formatting and seamless integration with front-end JavaScript code. This approach facilitated the organization of the final output into distinct categories encompassing various aspects. These categories include bias analysis in the article, information about the author, details about the website, and insights into the media company. Specifically, I ensured that the scripts would deliver analysis in multiple elements, allowing for collapsible/expandable windows and facilitating a more user-friendly experience.

Moreover, I conducted thorough research and composed content related to bias analysis and conflicts of interest concerning the eight main media sources displayed on the Home Screen. This content fits in with a functionality that enables users to click on any media company logo and access directly all information about the potential infusion of bias in their articles, based on political and financial affiliations.

Lastly, in preparation for presentation day, I contributed to the design of the poster, paying particular attention to the wording and layout to effectively convey our message.


_________________________________________________________________________________________________________________________________________________________________________________________

Sprint 3 Updates:

Lucy Zimmerman: 


This week, I made significant progress on enhancing our codebase, with a particular focus on the user interface (UI) aspects of our project. My efforts were concentrated on integrating and updating the UI to create a more visually appealing and cohesive user experience. I incorporated a variety of fonts, colors, and shadow effects to define the UI's aesthetic, successfully integrating these elements with the help of expo-font. A major change was transforming the app's color scheme to black and white, which not only streamlined the look but also emphasized a more uniform style. Additionally, I dedicated time to designing and implementing a loading screen for Transparency GPT. This screen features a rotating logo and presents users with interesting facts about misinformation while they wait for analysis to complete. Improvements were also made to the copy and paste clipboard functionality, enhancing user interaction with our application.

Further refining the analysis process, I separated the results into two distinct sections: one focusing on the source and the other on the text content itself. On this page, I introduced a sliding scale that visually represents the bias level of the text as determined by our algorithm. This development, in tandem with Nico's transition from using Claude to GPT for analysis, significantly reduced the loading times, streamlining the user experience. I also prepared a script file, setup.sh, which automates the downloading of necessary dependencies, simplifying the setup process for new users. 

Looking ahead to next week, I plan to further enhance the UI by adding sliding features for accessing detailed analyses of sources and topics. Additionally, I aim to redesign the result screen to include expansion arrows, offering users a more interactive way to explore specific details of the analysis, such as the author's identity, and so forth. These enhancements are aimed at providing a more intuitive and informative platform for our users.

Alberto Mancarella:

These few weeks, a major element in the code that I worked on was re-organizing the code and later adding more styling to the app, which Lucy expanded upon. I completely reorganized the code, where many react components now each belong in different files. When initially starting to  work on the styling, I found it too hard to keep track of all of the react components in a single file and do any meaningful modifications to a single specific component without affecting the whole app, both styling and functionality-wise. In addition, it was a bit tricky for group members to collaborate if we are all editing the same page. So, I knew that I had to split up the components into different files. I unfortunately had to re-design many of the functionalities that previous worked to expand it across different files, to allow for better modifications for each of the individual react components without heavily affecting each other.  For example, I now can edit the results view and how it looks without modifying the main view of the app. This also made it much easier to expand upon the styling later. Each style sheet contains way less elements, making it much easier to keep track of and modify the styling of individual components. In addition, I also spent time working on styling to allow users to have a much smoother experience while using the app, which Lucy expanded upon and improved later.  In addition, I added the toggle switch at the top of the screen to allow the user to switch between “URL” and “Article”. This results in the user being able to switch between the two modes in the app. In the URL article, for example, the same “paste URL” button appeared”. In the “Article” mode, there the two text inputs appeared (one for the article itself and one for the source). The appropriate text inputs get updated appropriately when the user switches between the two modes. Instead of simply printing to the console, I added a functionality where the error message would pop up if a user inputs an invalid article input (including an incorrect url, no input, ect.).  This does not crash the app, but instead asks the user to type in a valid input. Finally, I also added a scroll view in the results section of the app. This prevent text overflowing (if the results contains too much text that can’t all be displayed in the single screen).        


Something that I wish to expand upon is to add more functionalities and more user options in the main screen and to continue modifying the user interface. I would like to add the ability for the app to recommend some top articles for the day, for the user to select and analyze. I also would like to add the option for users simply click and analyize the top news sources, without even typing it in. In addition, for the final stretch of the project, I want to adjust and polish the user interface further. I ultimately want to continue to add more functionality and continue to edit the interface to ultimately allow for a better user experience! To accomplish this, I plan on exploring NewsAPI even further and continuing to study/implement various key concepts from react native.


Nicolas Friley:

During the third sprint, I explored the underlying reasons why our system was taking so long to output the final response. I accomplished this by timing each step of the process independently. This allowed me to identify the timing chokepoints. One of the major chokepoints I discovered was related to the component responsible for analyzing the bias present in the text, which was using Claude. While Claude was performing very well in its task, it was taking too much time to complete its analysis. To address this, I conducted a comparison between the timing of using GPT3.5 with equivalent prompts and examined the quality of the outputs. The quality of the outputs remained comparable, but the timing improvement was significant as we managed to save more than 16 seconds. Another chokepoint that contributed to the delay in generating the final response was the use of multiple Python scripts for prompting. These scripts were utilized to gather outputs from independent prompts using different LLMs and organize them into a cohesive and well-structured final response in a third script. However, this approach introduced additional time overhead. To overcome this, we decided to eliminate the intermediary step and feed each answer directly to the application, without combining them first. This modification resulted in another considerable reduction in processing time.

Moving forward, I intend to further refine our system by generalizing this approach to each of our prompts, ensuring that their answers are passed independently to the application without any issues. This approach will enable us to structure the final output into several categories, encompassing various aspects such as the analysis of bias in the article, information about the author, details about the website, and insights into the media company. Additionally, I will work on generating informational texts related to each media company in the list that will be displayed on the landing page of our app. This will allow our users to click on any media company logo, and receive information about how their political and financial affiliations may infuse a bias in the articles they produce.



_________________________________________________________________________________________________________________________________________________________________________________________


Sprint 2 Updates:

Lucy Zimmerman:

Initially, my focus was on integrating the backend Python scripts, which were primarily designed for processing and generating responses through LLm’s, with our web application's frontend. To achieve this integration, I turned to Flask, a lightweight and versatile web framework for Python. My role involved experimenting with Flask's capabilities to serve as the bridge between the complex logic encapsulated in the Python scripts and the user-facing side of our application. I began by setting up a basic Flask server, defining routes that could receive requests from the frontend. These routes were configured to trigger the execution of specific Python scripts, effectively allowing the frontend to utilize the LLM's capabilities indirectly. The challenge was to ensure that the data flow between the frontend, Flask server, and Python scripts was both efficient and secure. I implemented JSON as the data exchange format, enabling structured and easy-to-parse communication. To further enhance the connection between the frontend and the backend, I utilized Express.js. Express served as the backbone of our application's server-side logic, handling HTTP requests and routing them appropriately. For the client-side requests, I chose Axios. This setup allowed us to retrieve responses from the Python scripts in a non-blocking manner, ensuring that our application remained responsive to user interactions. The integration of Express and Axios required careful configuration, particularly in terms of setting up the correct endpoints, handling cross-origin resource sharing (CORS) issues, and managing asynchronous data flow. I dedicated time to testing and debugging this setup to guarantee smooth communication between the frontend, Express server, and Flask backend. Another critical aspect of my work involved optimizing the interaction with Claude and GPT models. My objective was to streamline the process of generating and handling prompts for these models, ensuring that we could dynamically generate responses based on user input. I implemented a system for token counting within each prompt, a crucial feature given the cost implications associated with the number of tokens processed by LLMs.  Lastly, I developed some of basic user interface (UI) for our application, including buttons, and the screen of the presentation of the subjectivity in an intuitive and engaging manner


A significant enhancement I aim to work on during sprint 2 is the automation of the article-to-app flow, leveraging the iPhone's share functionality. This will enable users to directly share links from their browser to our app, thereby initiating the article analysis process without the need for manual input. To achieve this, I'll be working with the iOS Share Extension.  To facilitate a smoother setup for demos and new users, I'll be developing a consolidated script that automates the installation of all necessary dependencies. Another exciting goal is the integration of the GPT prompt-pipeline directly within the app, focusing on evaluating the author, organization, and other contextual elements of the articles. This feature aims to provide users with deeper insights into the content, such as identifying biases, understanding the background of the author, and the credibility of the publishing organization. Implementing this requires fine-tuning our GPT prompts, using our token counter, and post-processing the prompts and also utilizing flask and express. I'll be exploring advanced NLP techniques and GPT's latest features to improve the quality of our analyses. Lastly, my efforts will be directed to UI improvements. The objective is to not only make the app more aesthetically pleasing but also to enhance usability and accessibility.





Alberto Mancarella:

My focus in these two weeks was to add more functionality to the app. Once Lucy built the bridge between the back-end and front-end server using the Flask server, I expanded upon this implementation. Instead of having a hardcoded text as an example, I expanded this by giving the system the ability to analyze any article that a user inputs. I added two ways that a user can search for an article. The first way is by pasting in a URL. To do so, the first way I solved is by adding a URL textbox, and a "Paste URL" button that inserts the URL that the user has copied into the URL textbox using the expo-clipboard library, and a "Analyze URL" button that analyzes the article specified by the URL inserted. The other way that a user can search for an article is by typing in the "Article Title" and "News Source". To implement this, I added "Article Title" and "News Source" text box that allows the user to input an article without having a URL. This is followed by a "Analyze Selected Source" that analyzes the article with the "Article Title" and "News Source" from the text-box. I added upon the JSON data exchange pipline from the front-end to the python script by indicicating if the user selected the "Analyze URL" or "Analyze Selected Source" button to efficiently to use the similar code later in the Python script. In addition, in this JSON data exchange pipline, I added all of the text-boxes information (such as "Article Title", "URL,", ect., so it can all be accessed in the back-end. In the back-end section of the code, I added the ability to fetch the actual article text so it can be inputted into the LLM pipline. In the scenerio where the user typed the "Analyze Selected Source" button, I utilized the NewsAPI API. I spent some time studying the NewsAPI and implemented it for this purpose. I utilized NewsAPI by essentially search for the most recent article given the specified parameters ("Article Title" and "News Source"). The actual content in the NewsAPI is heavily limited by the number of characters. So, I instead decided to get the URL instead. 
In the scenerio where the user typed in typed in the "Analyze URL" button, I didn't need to do any more processing since I already had the actual URL. I implemented a web-scraper using the BeautifulSoup library that takes in the URL as an input, and finally returns the scraped text, that gets inputted to the LLM.

For the next milestones, I feel like there are many improvements to be made. First of all, I plan on helping out with the ability to use the share feature, perhaps on top of all of the various input selections available currently. Since the variety of input choices has been already implementing, I plan on facilitating how the users actually input the text and navigate the app, experimenting with the app user-interface with multiple views, toggle switches, ect. I plan on playing around with various app interface to provide the user with the best experience and utilize the same functions that I already implemented. I expect to review material from CS 147L (a class that taught react native) to implement the best interface that is seemless and efficient for users.




Nicolas Friley:

During Sprint 2, my focus was on developing the backend Python scripts responsible for handling prompts to various Language Models and combining their responses into a well-structured analysis. I worked on three main files: claude_prompt.py and GPT_prompt.py, with a final integration in combined_prompt.py. In claude_prompt.py, I designed a function that takes the text of a target article as input. Using a series of chained prompts, this function generates a contextual bias analysis. The output includes a list of topics covered in the article, a subjectivity score ranging from 0 to 10, and the actual analysis. In GPT_prompt.py, I developed a function that leverages the list of topics, the URL and title of the article, and the author's name. By employing specific prompts, this function generates multiple independent and targeted bias analyses related to the author, media company, target audience, and more. I brought everything together with combined_prompt.py, which calls the functions from both claude_prompt.py and GPT_prompt.py and includes one final prompt to GPT to structure and format the final answer. For testing purposes, I implemented a simple text import function to read input data from a JSON file containing the author, URL, title, and text of the article. This allowed me to test the functions independently and ensure their individual functionality.

Looking ahead to Sprint 3, my plan is to upgrade the model used in GPT_prompt.py to utilize gpt-4. This will enhance the response generation capabilities, particularly for web-accessed information. I also intend to measure the response time of all three scripts separately and work on reducing the overall response time. Optimizing the prompts is another important task in Sprint 3. I aim to ensure that the final answer has the desired format and contains the required information, regardless of the media source. To achieve this, I will test the entire pipeline on articles from various sources and make the system robust to variations in content, access to information, publication dates, and other potential factors.



_________________________________________________________________________________________________________________________________________________________________________________________

Sprint 1 Updates

Lucy Zimmerman: 

During the first sprint, my focus was on conducting extensive research using ChatGPT. I invested significant time in prompting GPT with various URLs and exploring questions such as "detect any biased language" and "what is some helpful context for an informed reader on this news outlet." However, I encountered a challenge where some URLs were resistant to AI scraping. To address this, I created a Python program capable of retrieving web pages, converting them into PDFs, and extracting vital information like the author's name and main text. This development significantly enhanced our data acquisition process. I also began setting up an initial prompt with the OpenAI API to integrate these downloaded PDFs into our workflow. This setup was only partially completed due to the pending acquisition of API keys. Meanwhile, our team convened to discuss the potential of using Claude for more effective analysis of the articles themselves.

Looking ahead to the next sprint, my primary goal is to define the specific information and prompts that will be used with Claude to elicit the desired responses. Additionally, I aim to integrate our various Python programs, creating a cohesive system that combines data retrieval, contextual analysis using OpenAI, and bias detection through Claude. Another key objective is to contribute to the development of the user interface for this program. I plan to focus on seamlessly integrating the front-end and back-end components to ensure a functional and user-friendly experience. This comprehensive approach will not only streamline our processes but also enhance the overall efficacy of our project.

Rachel Liu:
The past two weeks, my main responsibility was conducting research into prompt engineering using ChatGPT– testing its capabilities with extracting main ideas, generating context (surrounding the context, author, organization, etc.), detecting charged language, and verifying sources. In particular, figuring out the most efficient way to chain prompts together and hone the specificity in our prompts to maximize the information that chatGPT is able to provide. In identifying its limitations, my team and I looked into other potential supplementary tools such as Claude and NewsAPI. More research into combining these sources must be done in order to figure out how we can utilize and merge all responses to generate the most nuanced and accurate response. 

In the next sprint, I will focus on developing the React App. I will research and create UI designs that best fit the features of TransparencyGPT and implement it in Typescript. Developing an effective user interface to match our backend developments will pull our project out of the ideation stage and provide a solid foundation from which we can build upwards. I will also explore with integrating WebSocket for real-time data exchange between the server and the front end and ensure the responsiveness of our application. By next sprint, we hope to have a fully functional application with preliminary backend functionality.

Nicolas Friley:

During the first sprint, I tested extensively the capabilities of GPT4 to extract information from user-generated content websites such as Quora, media outlet websites such as Fox News and commercial websites doing product recommendations and rankings. The tests were aiming at extracting political or financial affiliations and possible biases and conflicts of interest, playing with efficient follow up prompts and identifying best ”combined” prompts to get the most information in the least amount of prompts. I identified several websites containing specific bias and affiliations information about most media outlets, and plan on leveraging them in cross-reference when prompting GPT4. The idea is to develop a solution that manages to generalize well no matter the source URL. In one case, GPT4 used information from a survey that was focusing only on that one website's audience. Letting GPT4 use this kind of information instead of specifying the location of possible relevant resources makes it particularly hard to generalize to other websites for which such survey may not have been conducted.

After testing Claude as well on several articles to get it to output the desired type of information, I suggested mitigating the limits of GPT4 from our research findings by keeping GPT4 for extracting information through analysis of the website’s URL, author’s name, company information, and fact-checking, but switching to using Claude for bias analysis from the actual text after realizing the limited capabilities of GPT4 in this regard. For the next sprint, I will focus on optimizing prompts and sequences of prompts on targeted content, to generate relevant information of different nature from both GPT4 and Claude. The key idea is to explore how generated content from one API can be integrated efficiently into prompts for another, in order to get the best of both worlds. I will also work on prompts to format the final output and prune out unwanted type of information.


Alberto Mancarella:

In these past two weeks, I extensively conducted research on the Anthropic workbench (which uses the claude-2.1 model) from a variety of sources, also testing potential inputs and desired outputs. I played around with different input types, including asking about questions without context, asking questions with a website URL (in most cases it wasn't able to access websites, with the exception of wikipedia site and a few others), and asking questions with context simply as a text. Then, after this, I experimented with transfering this into code in a python script (using the Claude API). I also played around with prompt chaining (where I would ask the system a question, and then I would take the output of that into another prompt, asking to simplify the previous answer). In addition, I also researched potential ways to get up-to-date news. After some searching, I found NewsAPI. I decided to spend some time experimenting with NewsAPI, using a Python script. 

For the next sprint, I plan on helping to implement the pipeline where the main topics of the user input is recognized and fed into the NewsAPI, where the top articles regarding that subject are retrieved. Essentially, instead of providing the "context" when utilizing prompt chaining, I hope to automate the pipline where NewsAPI would get this context. In addition, I also plan on inplementing a basic mobile app using react native. I hope to implement some basic features, like text input, and then connect this to the pipline that the whole group is working on. 
