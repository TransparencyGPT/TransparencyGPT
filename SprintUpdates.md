Sprint 2 Updates:

Lucy Zimmerman:

Initially, my focus was on integrating the backend Python scripts, which were primarily designed for processing and generating responses through LLm’s, with our web application's frontend. To achieve this integration, I turned to Flask, a lightweight and versatile web framework for Python. My role involved experimenting with Flask's capabilities to serve as the bridge between the complex logic encapsulated in the Python scripts and the user-facing side of our application. I began by setting up a basic Flask server, defining routes that could receive requests from the frontend. These routes were configured to trigger the execution of specific Python scripts, effectively allowing the frontend to utilize the LLM's capabilities indirectly. The challenge was to ensure that the data flow between the frontend, Flask server, and Python scripts was both efficient and secure. I implemented JSON as the data exchange format, enabling structured and easy-to-parse communication. To further enhance the connection between the frontend and the backend, I utilized Express.js. Express served as the backbone of our application's server-side logic, handling HTTP requests and routing them appropriately. For the client-side requests, I chose Axios. This setup allowed us to retrieve responses from the Python scripts in a non-blocking manner, ensuring that our application remained responsive to user interactions. The integration of Express and Axios required careful configuration, particularly in terms of setting up the correct endpoints, handling cross-origin resource sharing (CORS) issues, and managing asynchronous data flow. I dedicated time to testing and debugging this setup to guarantee smooth communication between the frontend, Express server, and Flask backend. Another critical aspect of my work involved optimizing the interaction with Claude and GPT models. My objective was to streamline the process of generating and handling prompts for these models, ensuring that we could dynamically generate responses based on user input. I implemented a system for token counting within each prompt, a crucial feature given the cost implications associated with the number of tokens processed by LLMs.  Lastly, I developed some of basic user interface (UI) for our application, including buttons, and the screen of the presentation of the subjectivity in an intuitive and engaging manner


A significant enhancement I aim to work on during sprint 2 is the automation of the article-to-app flow, leveraging the iPhone's share functionality. This will enable users to directly share links from their browser to our app, thereby initiating the article analysis process without the need for manual input. To achieve this, I'll be working with the iOS Share Extension.  To facilitate a smoother setup for demos and new users, I'll be developing a consolidated script that automates the installation of all necessary dependencies. Another exciting goal is the integration of the GPT prompt-pipeline directly within the app, focusing on evaluating the author, organization, and other contextual elements of the articles. This feature aims to provide users with deeper insights into the content, such as identifying biases, understanding the background of the author, and the credibility of the publishing organization. Implementing this requires fine-tuning our GPT prompts, using our token counter, and post-processing the prompts and also utilizing flask and express. I'll be exploring advanced NLP techniques and GPT's latest features to improve the quality of our analyses. Lastly, my efforts will be directed to UI improvements. The objective is to not only make the app more aesthetically pleasing but also to enhance usability and accessibility.

_________________________________________________________________________________________________________________________________________________________________________________________

Sprint 1 Updates

Lucy Zimmerman: 

During the first sprint, my focus was on conducting extensive research using ChatGPT. I invested significant time in prompting GPT with various URLs and exploring questions such as "detect any biased language" and "what is some helpful context for an informed reader on this news outlet." However, I encountered a challenge where some URLs were resistant to AI scraping. To address this, I created a Python program capable of retrieving web pages, converting them into PDFs, and extracting vital information like the author's name and main text. This development significantly enhanced our data acquisition process. I also began setting up an initial prompt with the OpenAI API to integrate these downloaded PDFs into our workflow. This setup was only partially completed due to the pending acquisition of API keys. Meanwhile, our team convened to discuss the potential of using Claude for more effective analysis of the articles themselves.

Looking ahead to the next sprint, my primary goal is to define the specific information and prompts that will be used with Claude to elicit the desired responses. Additionally, I aim to integrate our various Python programs, creating a cohesive system that combines data retrieval, contextual analysis using OpenAI, and bias detection through Claude. Another key objective is to contribute to the development of the user interface for this program. I plan to focus on seamlessly integrating the front-end and back-end components to ensure a functional and user-friendly experience. This comprehensive approach will not only streamline our processes but also enhance the overall efficacy of our project.

Rachel Liu:
The past two weeks, my main responsibility was conducting research into prompt engineering using ChatGPT– testing its capabilities with extracting main ideas, generating context (surrounding the context, author, organization, etc.), detecting charged language, and verifying sources. In particular, figuring out the most efficient way to chain prompts together and hone the specificity in our prompts to maximize the information that chatGPT is able to provide. In identifying its limitations, my team and I looked into other potential supplementary tools such as Claude and NewsAPI. More research into combining these sources must be done in order to figure out how we can utilize and merge all responses to generate the most nuanced and accurate response. 

In the next sprint, I will focus on developing the React App. I will research and create UI designs that best fit the features of TransparencyGPT and implement it in Typescript. Developing an effective user interface to match our backend developments will pull our project out of the ideation stage and provide a solid foundation from which we can build upwards. I will also explore with integrating WebSocket for real-time data exchange between the server and the front end and ensure the responsiveness of our application. By next sprint, we hope to have a fully functional application with preliminary backend functionality.

Nicolas Friley:

During the first sprint, I tested extensively the capabilities of GPT4 to extract information from user-generated content websites such as Quora, media outlet websites such as Fox News and commercial websites doing product recommendations and rankings. The tests were aiming at extracting political or financial affiliations and possible biases and conflicts of interest, playing with efficient follow up prompts and identifying best ”combined” prompts to get the most information in the least amount of prompts. I identified several websites containing specific bias and affiliations information about most media outlets, and plan on leveraging them in cross-reference when prompting GPT4. The idea is to develop a solution that manages to generalize well no matter the source URL. In one case, GPT4 used information from a survey that was focusing only on that one website's audience. Letting GPT4 use this kind of information instead of specifying the location of possible relevant resources makes it particularly hard to generalize to other websites for which such survey may not have been conducted.

After testing Claude as well on several articles to get it to output the desired type of information, I suggested mitigating the limits of GPT4 from our research findings by keeping GPT4 for extracting information through analysis of the website’s URL, author’s name, company information, and fact-checking, but switching to using Claude for bias analysis from the actual text after realizing the limited capabilities of GPT4 in this regard. For the next sprint, I will focus on optimizing prompts and sequences of prompts on targeted content, to generate relevant information of different nature from both GPT4 and Claude. The key idea is to explore how generated content from one API can be integrated efficiently into prompts for another, in order to get the best of both worlds. I will also work on prompts to format the final output and prune out unwanted type of information.


Alberto Mancarella:

In these past two weeks, I extensively conducted research on the Anthropic workbench (which uses the claude-2.1 model) from a variety of sources, also testing potential inputs and desired outputs. I played around with different input types, including asking about questions without context, asking questions with a website URL (in most cases it wasn't able to access websites, with the exception of wikipedia site and a few others), and asking questions with context simply as a text. Then, after this, I experimented with transfering this into code in a python script (using the Claude API). I also played around with prompt chaining (where I would ask the system a question, and then I would take the output of that into another prompt, asking to simplify the previous answer). In addition, I also researched potential ways to get up-to-date news. After some searching, I found NewsAPI. I decided to spend some time experimenting with NewsAPI, using a Python script. 

For the next sprint, I plan on helping to implement the pipeline where the main topics of the user input is recognized and fed into the NewsAPI, where the top articles regarding that subject are retrieved. Essentially, instead of providing the "context" when utilizing prompt chaining, I hope to automate the pipline where NewsAPI would get this context. In addition, I also plan on inplementing a basic mobile app using react native. I hope to implement some basic features, like text input, and then connect this to the pipline that the whole group is working on. 
